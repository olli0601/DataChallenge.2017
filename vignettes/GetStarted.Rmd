---
title: "How to get started"
date: "`r Sys.Date()`"
github_document:
    toc_dept: 1
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```


### Register with github

GitHub is a version control repository that is used very widely by Statisticians
for versioning and collaborating on code and data analysis, both publicly and
privately. Almost all of our stats group are using it. [If you have not done yet,
you need to get an account now to make a submission to this year's Data
Challenge.](https://www.imperial.ac.uk/admin-services/ict/self-service/research-support/research-support-systems/github/)


### Installation

To get started, type in `R`:
```{r, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
# install.packages("devtools")
devtools:::install_github("olli0601/DataChallenge.2017")
require(DataChallenge.2017)
?ihr
?train
```    

Loading the package makes the various data sets available. [They are listed under
the functions tab
above](https://olli0601.github.io/DataChallenge.2017/reference/index.html).
Take a look to learn their names and get a first feel for the data points in
there. All data sets have two common columns, an anonymised country code (`ISO`)
and   year of data collection (`YEAR`). You can also do:

```{r, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
require(DataChallenge.2017)
data(ihr)
?ihr
```  

### Quick survival guide

The very first step is to combine different data sets by country code and
year of data collection. The data sets are stored as a `data.table` object.
A `data.table` is very similar to a standard `R` `data.frame`, but **much
faster** and **you can do a lot more**. [Have a look at
one or two data table
tutorials
online](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html).
  
First hint: use `data.table`'s `merge` function by country code and year to
combine the data sets. This is much more efficient than `for` loops or
`lapply`.
  
Second hint: try using `data.table`'s `[...]` syntax. The code below tells
you what proportion of the data are missing for each column of the `pov` data
set. 
```{r, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
require(DataChallenge.2017)
data(pov)
df <- melt(pov, id.vars=c('ISO','YEAR'), stringsAsFactors=FALSE)
df[, {
		z <- length(value)
		z2 <-length(which(is.na(value)))
		list(N=z, N_NA=z2, P_NA= z2/z)
	 }, by=c('variable')]		
```

Third hint: there really are not that many data points in the epidemic training
data set to begin with, while there are a large number of covariates. The
challenge is to make the most in this **small n large p setting**. What
techniques could be appropriate for predicting the number of 2015 and 2016
outbreaks? Start with some regressions, and google your way.

Fourth hint: the epidemiologic outbreak data are counts, but you don t have to
worry about this too much for this year's Data Challenge. It is fine to make
predictions that are real values.

### Interested in even more data?

You should have plenty of data to get started. You are welcome to search and
add in any other data that you think are useful in building the best predictive
model. Here are a few suggestions:

1. [INFORM open-source risk assessment for humanitarian crises and
   disasters.](http://www.inform-index.org/INFORM-2017-Results-and-data)
2. [WHO data repository on health
   workforce.](http://apps.who.int/gho/data/node.main.HWFGROUPS?lang=en)  
3. [WHO data repository on health
   financing.](http://apps.who.int/gho/data/node.main.484)

 